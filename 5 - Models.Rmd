```{r message=FALSE, warning=FALSE}
library(gtsummary)
library(fastDummies)
library(scales)
library(e1071)
library(tidyverse)
library(ResourceSelection)
library(probably)
library(tree)
library(caret)
library(data.table)
library(factoextra)
library(gridExtra)
library(plyr)
library(corrplot)
library(DMwR)
library(C50)
library(gains) 
library(leaps)
library(pROC)
library(nnet)
library(rpart)
library(rpart.plot)
library(ROCR)
library(xgboost)
```

```{r}
sig_var <- read.csv("Data_files/sig_var.csv")
```


```{r}
sig_var$riduzione <- ifelse(sig_var$riduzione == "OFFERTA CONVENZIONE 28\x80", "OFFERTA CONVENZIONE 28", sig_var$riduzione)
sig_var$riduzione <- ifelse(sig_var$riduzione == "OFFERTA CONVENZIONE 33\x80", "OFFERTA CONVENZIONE 33", sig_var$riduzione)
sig_var$riduzione <- ifelse(sig_var$riduzione == "OFFERTA SU QUANTITATIVO 30\x80", "OFFERTA SU QUANTITATIVO 30", sig_var$riduzione)
sig_var$riduzione <- ifelse(sig_var$riduzione == "OFFERTA SU QUANTITATIVO 44\x80", "OFFERTA SU QUANTITATIVO 44", sig_var$riduzione)
sig_var$riduzione <- ifelse(sig_var$riduzione == "PASS 60 e VOUCHER OFFERTA 30 \x80 ", "PASS 60 e VOUCHER OFFERTA 30", sig_var$riduzione)

sig_var$riduzione <- as.factor(sig_var$riduzione)
sig_var$churn <- as.factor(sig_var$churn)
sig_var$cap <- as.factor(sig_var$cap)
```


```{r}
set.seed(1)
trainIndex <- createDataPartition(sig_var$churn, p=.8, list=FALSE)
train_set <- sig_var[trainIndex,]
test_set <- sig_var[-trainIndex,]
```

# C5.0

```{r}
set.seed(22)
model_boost <-C5.0(train_set[,-10], train_set[,10], trials=10)
model_boost
```

```{r}
p_boost <- predict(model_boost, test_set[,-10], type="prob" )
p_boost <- data.frame(p_boost)
target <- ifelse(as.numeric(test_set[,10]) == 1 ,0,1)
result_boost <- cbind.data.frame(p_boost$X1,target)
names(result_boost) <- c("score","target")
threshold <- 0.5
result_boost$prediction <- ifelse(result_boost$score >= thresold,1,0)
```

```{r}
c5.0_cm <- table(Predicted = result_boost$prediction, Actual = result_boost$target)
c5.0_cm
```


# SVM

```{r}
set.seed(17)
# Create the SVM model
svm_model <- svm(churn ~ ., data = train_set, kernel = "radial", probability=TRUE)
```

```{r}
# Make predictions on the test set
p_svm <- predict(svm_model, test_set, probability=TRUE)
p_svm <- data.frame(attr(p_svm, "probabilities"))
result_svm <- cbind.data.frame(p_svm$X1,target)
names(result_svm) <- c("score","target")
result_svm$prediction <- ifelse(result_svm$score >= thresold,1,0)
```

```{r}
svm_cm <- table(Predicted = result_svm$prediction, Actual = result_svm$target)
svm_cm
```

# XGBoost

```{r message=FALSE, warning=FALSE}
set.seed(222)


data <- train_set

# Split the data into features (X) and the target variable (y)
X <- data[, 1:(ncol(data)-1)]  # the target variable is in the last column
y <- as.numeric(data$churn)-1 # the target variable is in the last column

#transform factor into dummy
X <- dummy_cols(X,
                remove_first_dummy = TRUE)

X <- X %>% select(-riduzione, -cap)

# Convert the data to a DMatrix format required by XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X), label = y)

# Specify the XGBoost parameters
params <- list(
  objective = "binary:logistic",  # For binary classification
  eval_metric = "auc",        # Evaluation metric
  max_depth = 6,                  # Maximum tree depth
  eta = 0.2,                      # Learning rate
  nrounds = 100                   # Number of boosting rounds (trees)
)
# Train the XGBoost model
xgb_model <- xgboost(params = params, data = dtrain, nrounds = params$nrounds)

# Make predictions on a new dataset 
new_data <- test_set
X_new <- new_data[, -ncol(new_data)]
X_new <- dummy_cols(X_new,
                remove_first_dummy = TRUE)
X_new <- X_new %>% select(-riduzione, -cap)
new_data_matrix <- as.matrix(X_new) 
dtest <- xgb.DMatrix(data = new_data_matrix)

# Predict probabilities for the positive class (Class 1)
xgb.pred <- predict(xgb_model, new_data_matrix)

xgb.pred.class <- ifelse(xgb.pred > threshold, 1, 0)

#Confusion Matrix
xgb.cm <- table(Predicted = xgb.pred.class, Actual = new_data$churn)
xgb.cm
```

# Benefit Function

```{r}
benefit <- function(confMatrix) {
  tn <- confMatrix[1,1]; fn <- confMatrix[1,2]
  fp <- confMatrix[2,1]; tp <- confMatrix[2,2]
  
  tp.bnft <- 25            ; CLV <- -60
  offer_cost <- -10        ; tn.bnft <- 35 #renew subscription
  resp_rate_call <- 0.35   ; a_c_call <- -1
  resp_rate_mail <- 0.15   ; a_c_mail <- -0.15
  attrition_rate <- 0.3    ; budget <- 5000

  
  call_cost <- a_c_call*(tp + fp)
  
  mail_cost <- a_c_mail*(tp + fp)
  
  call_outcome <- (tp*resp_rate_call)*((1-attrition_rate)*(tp.bnft + offer_cost) + attrition_rate*CLV) + tp*a_c_call +
          fn*CLV + 
          tn*tn.bnft + 
          fp*resp_rate_call*offer_cost + fp*a_c_call
  
  
  mail_outcome <- (tp*resp_rate_mail)*((1-attrition_rate)*(tp.bnft + offer_cost) + attrition_rate*CLV) + tp*a_c_mail +
          fn*CLV + 
          tn*tn.bnft + 
          fp*resp_rate_mail*offer_cost + fp*a_c_mail

  ROI <- call_outcome + mail_outcome
  
  Remaining_budget <- budget + call_cost + mail_cost 
  
  total_revenues <- c(call_cost, mail_cost, call_outcome,mail_outcome, ROI, Remaining_budget)
   
   return(total_revenues)
}
```

# C5.0 Benefit

```{r}
c5.0_benefit <- data.frame(numeric(), numeric(), numeric(), numeric(), numeric(), numeric(), numeric())
for (i in seq(0,1,0.01)) {
  newValue <- factor(ifelse(result_boost$score > i, 1, 0), levels = levels(test_set$churn))
  cm <- table(predicted=newValue, actual=test_set$churn)
  c5.0_benefit <- rbind(c5.0_benefit,c(i, benefit(cm)))
}
colnames(c5.0_benefit) <- c("CutOff","Call Costs", "Mail Costs", "Call Revenues", "Mail Revenues", "ROI", "Remaining budget")

c5.0_benefit
# write.csv(c5.0_benefit, "images/c5.0_benefit.csv", row.names = F)


c5.0_ROI <- ggplot(c5.0_benefit) +
  aes(x = CutOff, y = ROI, colour = ROI, linewidth = ROI) +
  geom_line(linewidth=3, show.legend = FALSE) +
  # geom_point(aes(size = ROI, color = ROI)) +
  scale_y_continuous(labels = scales::comma) +
    theme_minimal() +
    scale_colour_viridis_c() +
  labs(
    x = "Threshold",
    y = "ROI",
    title = "C5.0's Return on Investment"
  )
# ggsave("images/C5.0_ROI.png", dpi = 600)
c5.0_ROI
```

# SVM Benefit

```{r}
options(scipen = 999)

svm_benefit <- data.frame(numeric(), numeric(), numeric(), numeric(), numeric(), numeric(), numeric())
for (i in seq(0,1,0.01)) {
  newValue <- factor(ifelse(result_svm$score > i, 1, 0), levels = levels(test_set$churn))
  cm <- table(predicted=newValue, actual=test_set$churn)
  svm_benefit <- rbind(svm_benefit,c(i, benefit(cm)))
}
colnames(svm_benefit) <- c("CutOff","Call Costs", "Mail Costs", "Call Revenues", "Mail Revenues", "ROI", "Remaining budget")

svm_benefit
# write.csv(svm_benefit, "images/svm_benefit.csv", row.names = F)



svm_ROI <- ggplot(svm_benefit) +
  aes(x = CutOff, y = ROI, colour = ROI, linewidth = ROI) +
  geom_line(linewidth=3, show.legend = FALSE) +
  # geom_point(aes(size = ROI, color = ROI)) +
  scale_y_continuous(labels = scales::comma) +
    theme_minimal() +
    scale_colour_viridis_c() +
  labs(
    x = "Threshold",
    y = "ROI",
    title = "SVM Return on Investment"
  )

# ggsave("images/SVM_ROI.png", dpi = 600)
svm_ROI
```

# XGBoost Benefit

```{r}
options(scipen = 999)

xgb_benefit <- data.frame(numeric(), numeric(), numeric(), numeric(), numeric(), numeric(), numeric())
for (i in seq(0,1,0.01)) {
  newValue <- factor(ifelse(xgb.pred > i, 1, 0), levels = levels(test_set$churn))
  cm <- table(predicted=newValue, actual=test_set$churn)
  xgb_benefit <- rbind(xgb_benefit,c(i, benefit(cm)))
}
colnames(xgb_benefit) <- c("CutOff","Call Costs", "Mail Costs", "Call Revenues", "Mail Revenues", "ROI", "Remaining budget")

xgb_benefit
# write.csv(xgb_benefit, "images/xgb_benefit.csv", row.names = F)



xgb_ROI <- ggplot(xgb_benefit) +
  aes(x = CutOff, y = ROI, colour = ROI, linewidth = ROI) +
  geom_line(linewidth=3, show.legend = FALSE) +
  # geom_point(aes(size = ROI, color = ROI)) +
  scale_y_continuous(labels = scales::comma) +
    theme_minimal() +
    scale_colour_viridis_c() +
  labs(
    x = "Threshold",
    y = "ROI",
    title = "XGBoost Return on Investment"
  )

# ggsave("images/XGBoost_ROI.png", dpi = 600)
xgb_ROI
```

```{r}
ggplot(test_set, aes(x=p_boost$X1, fill = churn, color = churn)) +
  geom_histogram(bins = 20, alpha = 0.5) +
  theme_minimal() +
  scale_fill_viridis_d(aesthetics = c('color', 'fill'), end = 0.8) +
  labs(title = 'Distribution of Churn Prediction Probabilities', x = 'Probability Prediction', y = 'Count')
```
# Sens/spec

```{r}
sens_spec_plot <- function(actual_value, positive_class_name, negitive_class_name, pred_probability){
  # Initialising Variables
  specificity <- c()
  sensitivity <- c()
  youden_index <- c()
  cutoff <- c()
  
  for (i in 1:10) {
    predList <- as.factor(ifelse(pred_probability >= i/10, positive_class_name, negitive_class_name))
    specificity[i] <- specificity(predList, actual_value)
    sensitivity[i] <- sensitivity(predList, actual_value)
    youden_index[i] <- specificity[i] + sensitivity[i] - 1
    cutoff[i] <- i/10
  }
  df.sens.spec <- as.data.frame(cbind(cutoff, specificity, sensitivity))
  
  ggplot(df.sens.spec, aes(x = cutoff)) +
    geom_line(aes(y = specificity, color = 'Specificity'), linewidth=1.5) +
    geom_line(aes(y = sensitivity, color = 'Sensitivity'), linewidth=1.5) +
    geom_line(aes(y = youden_index, color = "J index"), linewidth=1.5) +
    theme_minimal() +
    scale_colour_viridis_d() +
    labs(x = 'Cutoff p value', y='Sens/Spec',  title = 'Sensitivity-Specificity plot',fill = 'Plot') +
      theme_minimal()+ theme(legend.position="bottom")
}

sens_spec_plot(actual_value = test_set$churn, positive_class_name = '1', negitive_class_name = '0', pred_probability = xgb.pred)
```

```{r}
find_p_cutoff <- function(actual_value, positive_class_name, negitive_class_name, pred_probability, p_01=1, p_10=1){
  # Initialising Variables
  msclaf_cost <- c()
  youden_index <- c()
  cutoff <- c()
  P00 <- c() #correct classification of negative as negative (Sensitivity)
  P01 <- c() #misclassification of negative class to positive class (actual is 0, predicted 1)
  P10 <- c() #misclassification of positive class to negative class (actual 1 predicted 0)
  P11 <- c() #correct classification of positive as positive (Specificity)
  
  costs = matrix(c(0, p_01, p_10, 0), ncol = 2)
  
  for (i in 1:100) {
    predList <- as.factor(ifelse(pred_probability >= i/100, positive_class_name, negitive_class_name))
    tbl <- table(predList, actual_value)
    
    # Classifying actual no as yes
    P00[i] <- tbl[1]/(tbl[1] + tbl[2])
    
    P01[i] <- tbl[2]/(tbl[1] + tbl[2])
    
    # Classifying actual yes as no
    P10[i] <- tbl[3]/(tbl[3] + tbl[4])
    
    P11[i] <- tbl[4]/(tbl[3] + tbl[4])
    
    cutoff[i] <- i/100
    msclaf_cost[i] <- P10[i] * costs[3] + P01[i] * costs[2]
    youden_index[i] <- P11[i] + P00[i] - 1
  }
  df.cost.table <- as.data.frame(cbind(cutoff, P10, P01, P11, P00, youden_index, msclaf_cost))
  cat(paste0('The ideal cutoff for:\n Yodens Index approach : ', which.max(df.cost.table$youden_index)/100))
  cat(paste0('\n Cost based approach : ', which.min(df.cost.table$msclaf_cost)/100))
  ggplot(df.cost.table, aes(x = cutoff)) +
    geom_line(aes(y = youden_index, color = 'yoden index'), linewidth=1.5) +
    geom_line(aes(y = msclaf_cost, color = 'misclassification cost'), linewidth=1.5) +
    theme_minimal() +
    scale_colour_viridis_d(end = 0.8) +
    labs(x = 'Cutoff p value', y='Index',  title = 'Cutoff p value',fill = 'Plot') +
      theme_minimal()+ theme(legend.position="bottom")
}

find_p_cutoff(actual_value = test_set$churn, positive_class_name = '1', negitive_class_name = '0', pred_probability = xgb.pred, p_01 =1, p_10 = 1)
```
#Confusion Matrix

```{r}
draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Non-churner', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Churner', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Non-churner', cex=1.2, srt=90)
  text(140, 335, 'Churner', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  
```


```{r}
optimal_threshold <- 0.28
opt_t.xgb.pred.class <- ifelse(xgb.pred > optimal_threshold, 1, 0)
opt_t.xgb.cm <- confusionMatrix(as.factor(opt_t.xgb.pred.class), as.factor(new_data$churn), mode = "prec_recall")

ROI_threshold <- 0.44
ROI_xgb.pred.class <- ifelse(xgb.pred > ROI_threshold, 1, 0)
ROI_xgb.cm <- confusionMatrix(as.factor(ROI_xgb.pred.class), as.factor(new_data$churn), mode = "prec_recall")



draw_confusion_matrix(opt_t.xgb.cm)
draw_confusion_matrix(ROI_xgb.cm)


```


